{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fairness in Machine Learning**\n",
    "\n",
    "Fairness in Machine Learning is the ability to avoid or eliminate biases in machine learning models algorithms. human beings intentionally or unintentionally are general bias when it comes to building machine learning or AI systems.\n",
    "\n",
    "Main types of Fairness-related harms \n",
    "- Allocation\n",
    "- Quality of Services\n",
    "- Stereotype\n",
    "- Denigrating\n",
    "- over and under-representation\n",
    "  \n",
    "  - Allocation: Occurs as a result of favoritism of gender. An example is the case of Amazon scraping out AI recruitment software for favoring more males than females resulting in a gender imbalance.\n",
    "\n",
    "  - Quality of Services: Occurs as a result of the low quality of the Ml algorithm which in return is not able to identify objects or people. An example is a hand soap dispenser unable to identify people with dark skin. \n",
    "\n",
    "  - Stereotype: Occurs as a result of using a received idea to describe a person or people ignoring. An example is the use of \"He and She\" pronouns in Turkish is regarded as unwanted because Turkish use genderless language. \n",
    "\n",
    "  - Denigrating: Occurs as a result of using offensive or abusive words to describe an object, person, or people either based on race or ethnicity. An example is a black man identified as a chimpanzee from google search photos, due to the use of dependent on historical data.\n",
    "\n",
    "  - over and under-representation: Occurs as a result of low representation of one gender, ethnicity, or race over another. An example is the use of grid pictures to represent CEOs where males are dominant over females.\n",
    "\n",
    "\n",
    "**Groups of people likely to experience in Un-Fairness in ML**\n",
    "- users and stakeholders\n",
    "- Legally protected groups e.g people from any race, ethnicity, age, gender or disabled persons.\n",
    "- Historically Marginalized groups of people.\n",
    "\n",
    "Assessment method to Fairness in ML\n",
    "- Identify Harm and Benefits\n",
    "- Identify the affected groups\n",
    "- Define faireness metrics\n",
    "  \n",
    "**Mitigating unfairness in ML**\n",
    "To reduce or eliminate unfairness we use the Fairlearn python package  \n",
    "- Fairlearn: it is use to mitigate unfairness assess fairness in *Allocation Harms* and *Quality-of-service harms*.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
